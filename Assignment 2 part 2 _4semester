---
title: "Computational Modeling - Week 5 - Assignment 2 - Part 2"
author: "Riccardo Fusaroli"
date: "2/19/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## In this assignment we learn how to assess rates from a binomial distribution, using the case of assessing your teachers' knowledge of CogSci.

### Second part: Focusing on predictions

Last year you assessed the teachers (darned time runs quick!). Now you want to re-test them and assess whether your models are producing reliable predictions. In Methods 3 we learned how to do machine-learning style assessment of predictions (e.g. rmse on testing datasets). Bayesian stats makes things a bit more complicated. So we'll try out how that works. N.B. You can choose which prior to use for the analysis of last year's data.

Questions to be answered (but see guidance below):
1- Write a paragraph discussing how assessment of prediction performance is different in Bayesian vs. frequentist models
2- Provide at least one plot and one written line discussing prediction errors for each of the teachers.

This is the old data:
- Riccardo: 3 correct answers out of 6 questions
- Kristian: 2 correct answers out of 2 questions (then he gets bored)
- Josh: 160 correct answers out of 198 questions (Josh never gets bored)
- Mikkel: 66 correct answers out of 132 questions

This is the new data:
- Riccardo: 9 correct answers out of 10 questions (then he freaks out about teaching preparation and leaves)
- Kristian: 8 correct answers out of 12 questions
- Josh: 148 correct answers out of 172 questions (again, Josh never gets bored)
- Mikkel: 34 correct answers out of 65 questions

Guidance Tips

1. There are at least two ways of assessing predictions.
2. Last year's results are this year's expectations.
3. Are the parameter estimates changing? (way 1)
4. How does the new data look in last year's predictive posterior? (way 2)

```{r}

new_d= data.frame(
  name = c("Riccardo","Kristian", "Josh", "Mikkel"),
  cor_ans = c(9,8,148,34),
  questions = c(10,12,172,65)
  
)

# way 2 to answer question
# defining grid
p_grid = seq(from = 0, to = 1, length.out = 1000)
  
# defining prior (three different)
prior = dnorm(p_grid, mean = 0.8, sd = 0.2)
# function for getting predictive distribution
predictive_distribution = function(name, size){
  # computing likelihood at each value in grid
  likelihood = dbinom(df$cor_ans[full_d$name == name], full_d$questions[full_d$name == name], prob = grid)
  
  # compute product of likelihood and prior
  unstd.posterior = likelihood * prior
  
  # standardizing posterior, so it sums to 1
  posterior = unstd.posterior / sum(unstd.posterior)
  
  # sampling 
  samples = sample(p_grid, prob = posterior, size = 10000, replace = T)
  
  w = rbinom( 1e4 , size , prob = samples )#to see if we could actually predict their performance on the new CogSci test we set the size to the "new" size of the test, the number of the questions
  
  simplehist(w- 12)
} 

w <- rbinom(1e4, size=2, prob= 0.8)
simplehist(w)

simplehist(w)
table(w)/1e4
#we would not have expected Kristian to get only 8 right, anything above 8 would have been more plausible



```



```{r}

predictive_distribution(name = "Josh", size = 370)
predictive_distribution(name = "Kristian", size = 14)
predictive_distribution(name = "Riccardo", size = 16)
predictive_distribution(name = "Mikkel", size = 197)
```




```{r}
## way 1 to answer question
# new data set with this years data
full_d = df[,2:3] + new_d[,2:3]
full_d$name = new_d$name
#colnames(full_d) <- c("questions", "cor_ans", "names")
g_d = data.frame()
# calculating new posterior with the added data
# calculated knowledge of teachers using prior with mean 0.8 and sd 0.2
for (t in full_d$name){
  # defining grid
  p_grid = seq(from = 0, to = 1, length.out = 1000)
  
  # defining prior
  prior = dnorm(p_grid, mean = 0.8, sd = 0.2)
  prior = prior / sum(prior)
  # computing likelihood at each value in grid
  likelihood = dbinom(full_d$cor_ans[full_d$name == t], full_d$questions[full_d$name == t], prob = p_grid)
  
  # compute product of likelihood and prior
  unstd.posterior = likelihood * prior
  
  # standardizing posterior, so it sums to 1
  posterior = unstd.posterior / sum(unstd.posterior)
  
  # plotting 
  g_d = data.frame(grid = p_grid, posterior = posterior, prior = prior, likelihood = likelihood)
  plot = ggplot(g_d, aes(grid, posterior))+geom_point()+geom_line()+theme_classic()+
geom_line(aes(grid,prior),color='red')+ xlab(paste("Knowledge of",t))+ ylab("posterior probability") 
  print(plot)
}
```



2- Provide at least one plot and one written line discussing prediction errors for each of the teachers.
```{r}
library(rethinking)
#Kristian
p_grid <- seq( from=0 , to=1 , length.out=1000 ) #this is some strong overfitting; we have only 6 data9points = questions to draw our knowledge from
prior <-dnorm(p_grid,0.8,0.2) #flat prior = all 1s, = a thousand entries of 1s, takes the first value and repeats it a 1000 time
likelihood <- dbinom( 2 , size=2, prob=p_grid ) #it is binomial (only right or wrong is an answer) that's why we use dbinom
posterior <- likelihood * prior
posterior <- posterior / sum(posterior)
samplesT <- sample (p_grid, prob = posterior, size = 1e4, replace = TRUE)
w <- rbinom(1e4, size=2, prob= 0.8)
simplehist(w)
w <- rbinom(1e4, size =12, prob= samplesT) #to see if we could really predict their performance on the new test we set the size to the "new" size of the test thats our number of the questions
simplehist(w)
table(w)/1e4
#we would not expect Kristian to get only 8 right, anything above 8 would have been plausible 
#Riccardo
p_grid <- seq( from=0 , to=1 , length.out=1000 ) #this is some strong overfittingin cause we have only 6 data-points (questions) to draw our knowledge from
prior <-dnorm(p_grid,0.8,0.2)  #flat prior, thus all 1s, = a thousand entries of 1s, takes the first value and repeats it a 1000 time
likelihood <- dbinom( 3 , size=6, prob=p_grid ) #itsbinomial (only right or wrong is possible in the questions) that's why we use dbinom
posterior <- likelihood * prior
posterior <- posterior / sum(posterior)
samplesR <- sample (p_grid, prob = posterior, size = 1e4, replace = TRUE)
w <- rbinom(1e4, size=6, prob= 0.8)
simplehist(w)
w <- rbinom(1e4, size =10, prob= samplesR) #to see if we could predict their performance on the new  test we set the size to the "new" size of the test that is our number of the questions
simplehist(w)
table(w)/1e4
#Riccardo surprises and overwhelms our model a little, simply to good
#Mikkel
p_grid <- seq( from=0 , to=1 , length.out=1000 ) 
prior <- dnorm(p_grid,0.8,0.2) 
likelihood <- dbinom( 66 , size=132, prob=p_grid ) 
posterior <- likelihood * prior
posterior <- posterior / sum(posterior)
samplesM <- sample (p_grid, prob = posterior, size = 1e4, replace = TRUE)
w <- rbinom(1e4, size=66, prob= 0.8)
simplehist(w)
w <- rbinom(1e4, size =65, prob= samplesM) 
simplehist(w)
table(w)/1e4
#pretty good predictions for Mikkel, he is still about the same as last year as it seems
#JOSH
p_grid <- seq( from=0 , to=1 , length.out=1000 ) 
prior <- dnorm(p_grid,0.8,0.2) #centered around mean of 0.8, std of 0.2 
likelihood <- dbinom( 160 , size=198, prob=p_grid ) 
posterior <- likelihood * prior
posterior <- posterior / sum(posterior)
samplesJ <- sample (p_grid, prob = posterior, size = 1e4, replace = TRUE)
w <- rbinom(1e4, size=66, prob= 0.8)
simplehist(w)
w <- rbinom(1e4, size =172, prob= samplesJ) 
simplehist(w)
table(w)/1e4
#captured by last year's model too and we see that Josh's new score lies within our peak from the last year

```
